<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hee Jae Kim</title>

    <meta name="author" content="Hee Jae Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/globe.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hee Jae Kim
                </p>
                <p>
			I am a third-year Ph.D. student at Boston University, advised by Prof. <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>. My research interests lie in computer vision, robotics, and machine learning with their applications in autonomous and assistive systems.
                </p>
                <p>
                  	Prior to BU, I got my master's degree (2019-2021) at the Ewha Womans University, where I worked with Prof. <a href="https://home.ewha.ac.kr/~bulee/">Byung-Uk Lee</a> and Prof. <a href="https://sagittak.wixsite.com/icplab">Jewon Kang</a>, on multi-view 360-degree videos. 
                </p>
		      
                <p>
                  	In 2018, I worked as a research intern at the <a href="https://www.etri.re.kr/eng/sub6/sub6_0101.etri?departCode=10">Artificial Intelligence Research Center at ETRI</a> (Korea). In 2021, I worked as a researcher at <a href="https://rainbirdgeo.com/en/home-english/">RainbirdGEO</a>.
                </p>
		      
                <p style="text-align:center">
                  <a href="mailto:hjkim37@bu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=9i7QbK0AAAAJ&hl=en&oi=sra">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hee-jae-kim/">Github</a> &nbsp;/&nbsp;
                  <a href="files/HeejaeKim_CV_2024.pdf">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/heejae.JPG"><img style="width:70%; max-width:70%; object-fit:cover; border-radius:10%;" alt="profile photo" src="images/heejae.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  Coming soon.
                </p> -->
              </td>
            </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
<!--               <td style="padding:20px;width:25%;vertical-align:middle">  -->
	      <td style="padding:5px;width:25%;vertical-align:top">
                <div class="one">
                  <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
                  <source src="images/blindways.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
<!--                   <img src='images/MDN_teaser.jpg' width=100%>  -->
<!-- 		  <img src='images/mdn_arch.jpg' width=100%> -->
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }

                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://hee-jae-kim.github.io/">
                  <span class="papertitle">Text to Blind Motion</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                <br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
                <br>
                <a href="https://blindways.github.io/">project page</a>
                /
                <a href="https://blindways.github.io/">paper</a>
<!-- 		/ -->
<!-- 		<a href="https://blindways.github.io/">code</a>
		/ -->
<!-- 		<a href="files/MDN_cvpr24_poster.pdf">poster</a> -->
                <p></p>
                <p>
                  We introduce BlindWays, the first multimodal 3D human motion benchmark for pedestrians who are blind, featuring data from 11 participants (varying in gender, age, visual acuity, onset of disability, mobility aid use, and navigation habits) in an outdoor navigation study. We provide rich two-level textual descriptions informed by third-person and egocentric videos. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pretraining-based methods for our novel task.
                </p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nuvo_image'><video  width=115% muted autoplay loop>
                  <source src="images/MDN_anim.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/MDN_teaser.jpg' width=100%>
<!-- 		  <img src='images/mdn_arch.jpg' width=100%> -->
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }

                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://hee-jae-kim.github.io/">
                  <span class="papertitle">Motion Diversification Networks</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://mdncvpr.github.io/">project page</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">paper</a>
		/
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">code</a>
		/
		<a href="files/MDN_cvpr24_poster.pdf">poster</a>
                <p></p>
                <p>
                  We introduce Motion Diversification Networks, a novel framework for learning to generate realistic and diverse 3D human motion. Towards more realistic and functional 3D motion models, this work uncovers limitations in existing generative modeling techniques, particularly in overly simplistic latent code sampling strategies.
                </p>
              </td>
            </tr>
          </tbody></table>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/rainbirdgeo.jpeg' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">
                  <span class="papertitle">Unsupervised Clustering of Geostationary Satellite Cloud Properties for Estimating Precipitation Probabilities of Tropical Convective Clouds</span>
                </a>
                <br>
		Doyi Kim,
                <strong>Hee Jae Kim</strong>,
		Yong-Sang Choi
                <br>
                <em>Journal of Applied Meteorology and Climatology (JAMC)</em>, 2023
                <br>
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">paper</a>
                <p></p>
                <p>This study aims to explore the cloud properties of tropical convective clouds (TCC) that indicate a high probability of precipitation by training a neural network with daytime satellite imagery. </p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/360SR.jpg' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">
                  <span class="papertitle">360&deg; Image Reference-Based Super-Resolution using Latitude-Aware Convolution Learned from Synthetic to Real</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
		<a href="https://sagittak.wixsite.com/icplab">Jewon Kang</a>,
		<a href="https://home.ewha.ac.kr/~bulee/">Byung-Uk Lee</a>
                <br>
                <em>IEEE Access</em>, 2021
                <br>
                <a href="https://iamheejae.github.io/lat360.github.io/">project page</a>
                /
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">paper</a>
		/ 
		<a href="https://github.com/iamheejae/Lat360">code</a>
                <p></p>
                <p>We propose an efficient reference-based 360&deg; image super-resolution (RefSR) technique to exploit a wide field of view (FoV) among adjacent 360&deg; cameras. We do not assume any structured camera arrays but use a reference image captured in an arbitrary viewpoint. Accordingly, we develop a long-range 360 disparity estimator (DE360) to overcome a large and distorted disparity between equirectangular projection (ERP) images, particularly near the poles. </p>
              </td>
            </tr>
          </tbody></table>


        </td>
      </tr>
    </tbody></table>
  </body>
</html>
