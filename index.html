<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hee Jae Kim</title>

    <meta name="author" content="Hee Jae Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/globe.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hee Jae Kim
                </p>
                <p>
			I am a second-year Ph.D. student at Boston University, advised by Prof. <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>. My research interests lie in computer vision, robotics, and machine learning with their applications in autonomous and assistive systems.
                </p>
<!--                 <p>
                  Prior to BU, I worked with Prof. <a href="https://personal.utdallas.edu/~dzdu/">Dingzhu Du</a> as a research assistant, focusing on submodular function maximization problem (2019-2020). I received my master's degree in the Department of Mathematics at East China Normal University (ECNU), under the supervision of Prof. <a href="https://math.ecnu.edu.cn/~chlu/intro_c.html?language=2&id=116">Changhong Lu</a>, with a focus on Combinatorics and Graph Theory (2017-2020).
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:hjkim37@bu.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=9i7QbK0AAAAJ&hl=en&oi=sra">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;
                  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/hee-jae-kim/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/heejae.JPG"><img style="width:70%; max-width:70%; object-fit:cover; border-radius:10%;" alt="profile photo" src="images/heejae.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  Coming soon.
                </p> -->
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
                  <source src="images/MDN_anim.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/MDN_teaser.pdf' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }
        
                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ztransformer.github.io/">
                  <span class="papertitle">Motion Diversification Networks</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
<!--                 <a href="https://www.linkedin.com/in/zhongkai-leon-shangguan-52b465152">Zhongkai Shangguan*</a>,
                <a href="https://jimuyangz.github.io/">Jimuyang Zhang</a>, -->
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>,
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://ztransformer.github.io/">project page</a>
                /
<!--                 <a href="https://arxiv.org/pdf/2309.16772.pdf">paper</a> -->
                <p></p>
                <p>
                  We introduce Motion Diversification Networks, a novel framework for learning to generate realistic and diverse 3D human motion. Despite recent advances in deep generative motion modeling, existing models often fail to produce samples that capture the full range of plausible and natural 3D human motion within a given context. The lack of diversity becomes even more apparent in applications where subtle and multi-modal 3D human forecasting is crucial for safety, such as in robotics and autonomous driving. Towards more realistic and functional 3D motion models, this work uncovers limitations in existing generative modeling techniques, particularly in overly simplistic latent code sampling strategies. We then introduce a transformer-based diversification mechanism that learns to guide sampling in the latent space. The proposed attention-based module queries multiple stochastic samples to flexibly predict a diverse set of latent codes which are subsequently decoded into motion samples. Our proposed network achieves state-of-the-art diversity and accuracy prediction performance across a range of settings, particularly when used to forecast intricate in-the-wild 3D human motion within complex urban environments.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </tbody></table>
  </body>
</html>
