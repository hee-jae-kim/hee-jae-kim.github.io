<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hee Jae Kim</title>

    <meta name="author" content="Hee Jae Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/globe.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hee Jae Kim
                </p>
                <p>
					I am a third-year Ph.D. student at Boston University, advised by Prof. <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>. My research interests lie in computer vision, robotics, and machine learning with their applications in autonomous and assistive systems.
                </p>
                <p>
                  	Prior to BU, I got my master's degree (2019-2021) at the Ewha Womans University, where I worked with Prof. <a href="https://home.ewha.ac.kr/~bulee/">Byung-Uk Lee</a> and Prof. <a href="https://sagittak.wixsite.com/icplab">Jewon Kang</a>, on multi-view 360-degree videos. 
                </p>
<!-- 		      
                <p>
                  	In 2018, I worked as a research intern at the <a href="https://www.etri.re.kr/eng/sub6/sub6_0101.etri?departCode=10">Artificial Intelligence Research Center at ETRI</a> (Korea). In 2021, I worked as a researcher at <a href="https://rainbirdgeo.com/en/home-english/">RainbirdGEO</a>.
                </p> -->
		      
                <p style="text-align:center">
                  <a href="mailto:hjkim37@bu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=9i7QbK0AAAAJ&hl=en&oi=sra">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hee-jae-kim/">Github</a> 
<!--                   <a href="files/CV_HeejaeKim_2025.pdf">CV</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/heejae.JPG"><img style="width:70%; max-width:70%; object-fit:cover; border-radius:10%;" alt="profile photo" src="images/heejae.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


          <!-- News -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>

                  <!-- Inner news table -->
                  <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <!-- DATE: no left padding + enough width + nowrap -->
                        <td style="min-width:12ch;white-space:nowrap;vertical-align:top;padding:0 8px 8px 0;">June&nbsp;25&nbsp;2025</td>
                        <td style="vertical-align:top;padding:0 8px 8px 8px;">
                          Excited to join <strong>Amazon</strong> as an Applied Scientist Intern in Seattle!
                        </td>
                      </tr>
                      <tr>
                        <td style="min-width:12ch;white-space:nowrap;vertical-align:top;padding:0 8px 8px 0;">June&nbsp;11&nbsp;2025</td>
                        <td style="vertical-align:top;padding:0 8px 8px 8px;">
                          Thrilled to co-organize the fourth workshop
                          <a href="https://accessibility-cv.github.io/"><strong>AVA: Accessibility, Vision, and Autonomy Meet</strong></a> at CVPR 2025.
                        </td>
                      </tr>
                    </tbody>
                  </table>

                </td>
              </tr>
            </tbody>
          </table>

					
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:10px;width:25%;vertical-align:top; text-align:right;">
	      <a href="https://hee-jae-kim.github.io/"><img src="./images/BranchOut.gif" class="publogo" width="195" height ="110"></a>
		      
              </td>
              <td style="padding:10px;width:75%;vertical-align:top; text-align:left;">
                <a href="https://hee-jae-kim.github.io/">
                  <span class="papertitle">BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions</span>
                </a>
                <br>
	        <strong>Hee Jae Kim</strong>, 
		<a href="https://zekai-yin.github.io/">Zekai Yin</a>,
		<a href="https://leilai125.github.io/">Lei Lai</a>,
		<a href="https://www.linkedin.com/in/jason-lee-b3479a17b/">Jason Lee</a>, and
	        <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
	        <br>
	        <em>Conference on Robot Learning (CoRL)</em>, 2025
	        <br>
	        <a href="https://hee-jae-kim.github.io/">project page</a>
	        /
	        <a href="https://hee-jae-kim.github.io/">paper</a>
                <p></p>
	        <p style="text-align: justify;">
	         We introduce BranchOut, a GMM-based diffusion planner and multimodal benchmark, towards the goal of modeling and evaluating realistic, human-like decision-making in autonomous driving. To support this, we develop a human-in-the-loop, photorealistic simulation framework that enables scalable collection of diverse, reactive trajectories and facilitates fine-grained, human-aligned evaluation of multimodal realism. Our end-to-end framework explicitly captures the rich, multimodal distribution of plausible future behaviors using a compact GMM-based diffusion model, achieving state-of-the-art performance across both error-based and distributional metrics. 
<!-- 		 Together, our model and benchmark advance a more accurate, behaviorally-aligned understanding of planning quality in complex real-world scenarios. -->
	        </p>
              </td>
            </tr>
          </tbody></table>


		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:10px;width:25%;vertical-align:top; text-align:right;">
	      <a href="https://blindways.github.io/"><img src="./images/blindways_ex.gif" class="publogo" width="195" height ="110"></a>
		      
              </td>
              <td style="padding:10px;width:75%;vertical-align:top; text-align:left;">
                <a href="https://blindways.github.io/">
                  <span class="papertitle">Text to Blind Motion</span>
                </a>
                <br>
	        <strong>Hee Jae Kim</strong>, 
		<a href="https://diasengupta.github.io/">Kathakoli Sengupta</a>,
		<a href="https://www.masakikuribayashi.com/">Masaki Kuribayashi</a>,
		<a href="https://terpconnect.umd.edu/~hernisa/">Hernisa Kacorri</a>, and
	        <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
	        <br>
	        <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
	        <br>
	        <a href="https://blindways.github.io/">project page</a>
	        /
	        <a href="https://arxiv.org/pdf/2412.05277">paper</a>
		/
		<a href="https://drive.google.com/drive/folders/1UHtQwS6-JMnqHMP6q11nub-BR2lBrZYv">data</a>
                <p></p>
	        <p style="text-align: justify;">
	          We introduce BlindWays, the first multimodal 3D human motion benchmark for pedestrians who are blind, featuring data from 11 participants (varying in gender, age, visual acuity, onset of disability, mobility aid use, and navigation habits) in an outdoor navigation study. We provide rich two-level textual descriptions informed by third-person and egocentric videos. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pretraining-based methods for our novel task.
	        </p>
              </td>
            </tr>
          </tbody></table>


		

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:10px;width:25%;vertical-align:top; text-align:right;">
	      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"><img src="./images/MDN_s.gif" class="publogo" width="195" height ="110"></a>    
              </td>
              <td style="padding:10px;width:75%;vertical-align:top; text-align:left;">
                <a href="https://mdncvpr.github.io/">
                  <span class="papertitle">Motion Diversification Networks</span>
                </a>
		      
                <br>
                <strong>Hee Jae Kim</strong> and
                <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://mdncvpr.github.io/">project page</a>
                /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">paper</a>
		/
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">code</a>
		/
		<a href="files/MDN_cvpr24_poster.pdf">poster</a>
                <p></p>
                <p style="text-align: justify;">
                 We introduce Motion Diversification Networks, a novel framework for learning to generate realistic and diverse 3D human motion. Towards more realistic and functional 3D motion models, this work uncovers limitations in existing generative modeling techniques, particularly in overly simplistic latent code sampling strategies.
                </p>
              </td>
            </tr>
          </tbody></table>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:10px;width:25%;vertical-align:top; text-align:right;">
		<a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml"><img src="./images/rainbirdgeo.jpeg" class="publogo" width="195" height ="110"></a>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top; text-align:left;">
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">
                  <span class="papertitle">Unsupervised Clustering of Geostationary Satellite Cloud Properties for Estimating Precipitation Probabilities of Tropical Convective Clouds</span>
                </a>
                <br>
		Doyi Kim,
                <strong>Hee Jae Kim</strong>, and
		Yong-Sang Choi
                <br>
                <em>Journal of Applied Meteorology and Climatology (JAMC)</em>, 2023
                <br>
                <a href="https://journals.ametsoc.org/view/journals/apme/62/8/JAMC-D-22-0175.1.xml">paper</a>
                <p></p>
                <p style="text-align: justify;">
		We propose a framework for identifying tropical convective clouds (TCCs) with high precipitation probabilities using geostationary satellite imagery. Our findings show that specific cloud features—such as colder tops and larger particle sizes—consistently signal precipitation likelihood across different regions and seasons. Our method enhances satellite-based precipitation forecasting, especially in tropical regions with limited ground-based observations.
		</p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:10px;width:25%;vertical-align:top; text-align:right;">
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634"><img src="./images/360SR.jpg" class="publogo" width="195" height ="110"></a>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top; text-align:left;">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">
                  <span class="papertitle">360&deg; Image Reference-Based Super-Resolution using Latitude-Aware Convolution Learned from Synthetic to Real</span>
                </a>
                <br>
                <strong>Hee Jae Kim</strong>,
		<a href="https://sagittak.wixsite.com/icplab">Jewon Kang</a>, and
		<a href="https://home.ewha.ac.kr/~bulee/">Byung-Uk Lee</a>
                <br>
                <em>IEEE Access</em>, 2021
                <br>
                <a href="https://iamheejae.github.io/lat360.github.io/">project page</a>
                /
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9617634">paper</a>
		/ 
		<a href="https://github.com/iamheejae/Lat360">code</a>
                <p></p>
		<p style="text-align: justify;">
                We propose an efficient reference-based 360&deg; image super-resolution (RefSR) technique to exploit a wide field of view (FoV) among adjacent 360&deg; cameras. We do not assume any structured camera arrays but use a reference image captured in an arbitrary viewpoint. Accordingly, we develop a long-range 360 disparity estimator (DE360) to overcome a large and distorted disparity between equirectangular projection (ERP) images, particularly near the poles. 
		</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Service</h2>
		      
                <p>
                  Reviewer
                  <ul>
                    <li>CoRL2025, NeurIPS2025</li>
                  </ul>
                </p>
		  
                <p>
                  Organizer
                  <ul>
                    <li>CVPR2024 AVA Accessibility Vision and Autonomy Challenge</li>
                    <li>CVPR2025 AVA Accessibility Vision and Autonomy Challenge</li>
                  </ul>
                </p>
		  
              </td>
            </tr>
          </tbody></table>
	  

        </td>
      </tr>
    </tbody></table>
  </body>
</html>
